{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980d6eec-966c-41ce-9e76-319e44e15b99",
   "metadata": {},
   "source": [
    "# Mistral 7b function calling trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd202461-2e1a-446f-bfd8-3f4f010d3909",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82e4d8-b233-4cc1-a253-273870f94342",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre --upgrade bigdl-llm[all]\n",
    "!pip install transformers==4.36.1\n",
    "!pip install peft==0.5.0\n",
    "!pip install datasets\n",
    "!pip install accelerate==0.23.0\n",
    "!pip install bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a9063-c148-413d-bca3-049e71c6b7d8",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bac155-b909-4d76-b933-b99fbade05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"]=\"./data/cache\"\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from bigdl.llm.transformers.low_bit_linear import LowBitLinear\n",
    "from bigdl.llm.transformers.qlora import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "from bigdl.llm.utils.isa_checker import ISAChecker\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb7b31-ede2-46a7-ac39-8e5a05b4e659",
   "metadata": {},
   "source": [
    "## Initialise Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aabff2-4e45-494c-8b84-423d57ad2e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_bos_token=True, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"int4\",  # nf4 not supported on cpu yet\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77085e48-cb6c-4d47-a815-23f9ca8d18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83dd04f-5bfd-438c-97b5-f9beef90d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_linear_layers(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LowBitLinear):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return list(lora_module_names)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=get_all_linear_layers(model),\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82912ffc-0281-45d6-bb41-73fa241a6bd7",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34915631-28e2-4b92-8764-e7c6ae6c7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glaiveai/glaive-function-calling-v2\", split='train')\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320138a-6b7c-4eef-9ebe-08fe49b155fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfa688-7873-4b73-8bbf-a791fd59b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_format = \"<|im_start|>System\\n{sys_msgs}<|im_end|>\\n{conversation_msgs}\"\n",
    "eval_prompt_format = \"[INST] {prompt} [/INST]\"\n",
    "\n",
    "def postprocess_conversation_msgs(msgs):\n",
    "    # Assuming user input is a string\n",
    "    lines = msgs.strip().split('\\n')\n",
    "    processed_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('USER:'):\n",
    "            user_message = line.replace('USER:', '').strip()\n",
    "            processed_lines.append(f\"<|im_start|>user\\n{user_message}<|im_end|>\")\n",
    "        elif line.startswith('ASSISTANT: <functioncall>'):\n",
    "            # Extracting function call information\n",
    "            function_call = line.replace('ASSISTANT: <functioncall>', '').strip()\n",
    "            function_call = function_call.replace('<|endoftext|>', '').strip()\n",
    "            processed_lines.append(f\"<|im_start|>function\\n{function_call}<|im_end|>\")\n",
    "        elif line.startswith('FUNCTION RESPONSE:'):\n",
    "            # Extracting function response information\n",
    "            function_response = line.replace('FUNCTION RESPONSE:', '').strip()\n",
    "            processed_lines.append(f\"<|im_start|>function_response\\n{function_response}<|im_end|>\")\n",
    "        elif line.startswith('ASSISTANT:'):\n",
    "            # Extracting assistant response\n",
    "            assistant_message = line.replace('ASSISTANT:', '').strip()\n",
    "            assistant_message = assistant_message.replace('<|endoftext|>', '').strip()\n",
    "            processed_lines.append(f\"<|im_start|>assistant\\n{assistant_message}<|im_end|>\")\n",
    "\n",
    "    return '\\n'.join(processed_lines)\n",
    "    \n",
    "def format_and_tokenize_prompt(tokenizer, data, max_length=512):\n",
    "    system_text, system_msgs = data['system'].split('SYSTEM:', 1)\n",
    "    system_msgs = system_msgs.strip()\n",
    "    # Printing the results\n",
    "    conversation_msgs = postprocess_conversation_msgs(data['chat'])\n",
    "    train_prompt = train_prompt_format.format(sys_msgs=system_msgs, conversation_msgs=conversation_msgs)\n",
    "    tokenized_prompt = tokenizer(train_prompt, max_length=max_length, truncation=True)\n",
    "    return tokenized_prompt\n",
    "    \n",
    "train_dataset = dataset['train'].map(lambda samples: format_and_tokenize_prompt(tokenizer, samples))\n",
    "test_dataset = dataset['test'].map(lambda samples: format_and_tokenize_prompt(tokenizer, samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c8cfee-d807-4c04-b646-786975529238",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9653a2-4cb4-42e2-967a-6bad39a8f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "isa_checker = ISAChecker()\n",
    "bf16_flag = isa_checker.check_avx512()\n",
    "trainer_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./data/checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_steps=0,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"adamw_hf\",\n",
    "    gradient_checkpointing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f91b8e-6251-4cf3-9c54-54e5ae5927ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    args=trainer_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b10d33-cb9d-4b3c-a891-1ab8b1f0c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.train()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8344f6-b6dc-4979-ad94-f7af49845d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
